# myTest.py
import torch
from transformers import AutoTokenizer
from config import Config
from model.Transformer import Transformer

class ModelDecoderTest:
    def __init__(self):
        self.tokenizer = AutoTokenizer.from_pretrained(Config.model_name)
        self.pad_id = self.tokenizer.pad_token_id
        self._init_model()
        self._verify_special_tokens()

    def _init_model(self):
        """åŠ è½½æ¨¡å‹å’Œæƒé‡"""
        self.model = Transformer(
            pad_idx=self.pad_id,
            enc_voc_size=len(self.tokenizer.get_vocab()),
            dec_voc_size=len(self.tokenizer.get_vocab()),
            d_model=Config.d_model,
            max_len=Config.max_len,
            batch_size=Config.batch_size,
            n_head=Config.n_head,
            n_layers=Config.n_layers,
            ffn_hidden=Config.ffn_hidden,
            drop_prob=Config.drop_prob,
            device=Config.device
        ).to(Config.device)
        
        # åŠ è½½é¢„è®­ç»ƒæƒé‡
        try:
            state_dict = torch.load('best_model.pt', map_location=Config.device)
            
            # ä¿®å¤é”®åä¸åŒ¹é…é—®é¢˜
            new_state_dict = {}
            for k, v in state_dict.items():
                if k.startswith('_orig_mod.'):
                    new_k = k.replace('_orig_mod.', '')
                    new_state_dict[new_k] = v
                else:
                    new_state_dict[k] = v
            
            # ä¸¥æ ¼æ¨¡å¼åŠ è½½
            missing_keys, unexpected_keys = self.model.load_state_dict(new_state_dict, strict=False)
            
            if missing_keys:
                print(f"âš ï¸ ç¼ºå¤±çš„é”®: {missing_keys}")
            if unexpected_keys:
                print(f"âš ï¸ æ„å¤–çš„é”®: {unexpected_keys}")
                
            print("âœ… æˆåŠŸåŠ è½½å¹¶ä¿®å¤ best_model.pt æƒé‡")
        except Exception as e:
            print(f"âŒ åŠ è½½æ¨¡å‹å¤±è´¥: {str(e)}")
            raise

    def _verify_special_tokens(self):
        """éªŒè¯ç‰¹æ®ŠToken"""
        print("\n=== ç‰¹æ®ŠTokenéªŒè¯ ===")
        print(f"EOS: {self.tokenizer.eos_token} (ID: {self.tokenizer.eos_token_id})")
        print(f"PAD: {self.tokenizer.pad_token} (ID: {self.pad_id})")
        assert self.tokenizer.eos_token_id is not None, "EOS Tokenå¿…é¡»å­˜åœ¨"

    def test_decoding_termination(self):
        """æµ‹è¯•è§£ç ç»ˆæ­¢æ¡ä»¶"""
        print("\n=== ç»ˆæ­¢ç¬¦æµ‹è¯• ===")
        src_text = "Eine Gruppe fÃ¼llt WasserbehÃ¤lter."
        src = self.tokenizer(src_text, return_tensors='pt')['input_ids'].to(Config.device)
        
        # åˆå§‹åŒ–decoderè¾“å…¥ï¼ˆç”¨EOSå¼€å¤´ï¼‰
        decoder_input = torch.full((1, Config.max_len), self.pad_id, device=Config.device)
        decoder_input[0, 0] = self.tokenizer.eos_token_id
        
        # æ¨¡æ‹Ÿè‡ªå›å½’ç”Ÿæˆ
        for i in range(1, Config.max_len):
            with torch.no_grad():
                output = self.model(src, decoder_input[:, :i])
                next_token = output.argmax(dim=-1)[:, -1]
                decoder_input[0, i] = next_token
            
            # å®æ—¶æ‰“å°ç”Ÿæˆç»“æœ
            current_output = self.tokenizer.decode(decoder_input[0, :i+1], skip_special_tokens=True)
            print(f"Step {i}: {current_output}")
            
            # æ£€æŸ¥æ˜¯å¦ç”ŸæˆEOSæå‰ç»ˆæ­¢
            if next_token == self.tokenizer.eos_token_id:
                print("ğŸ›‘ æ£€æµ‹åˆ°EOSï¼Œæå‰ç»ˆæ­¢ç”Ÿæˆ")
                break
        
        assert self.tokenizer.eos_token_id in decoder_input[0], "ç”Ÿæˆç»“æœæœªåŒ…å«EOSç»ˆæ­¢ç¬¦"

    def test_repetition_control(self):
        """æµ‹è¯•é‡å¤ç”Ÿæˆæ§åˆ¶"""
        print("\n=== é‡å¤ç”Ÿæˆæµ‹è¯• ===")
        src_text = "Describe a landscape"
        src = self.tokenizer(src_text, return_tensors='pt')['input_ids'].to(Config.device)
        
        decoder_input = torch.full((1, Config.max_len), self.pad_id, device=Config.device)
        decoder_input[0, 0] = self.tokenizer.eos_token_id
        
        generated_tokens = set()
        repeated_count = 0
        
        for i in range(1, Config.max_len):
            with torch.no_grad():
                output = self.model(src, decoder_input[:, :i])
                next_token = output.argmax(dim=-1)[:, -1]
                decoder_input[0, i] = next_token
            
            # æ£€æŸ¥é‡å¤Token
            token = next_token.item()
            if token in generated_tokens:
                repeated_count += 1
                print(f"âš ï¸ é‡å¤Token: {self.tokenizer.decode([token])} (ID: {token})")
            generated_tokens.add(token)
            
            if repeated_count > 3:
                raise AssertionError("æ£€æµ‹åˆ°è¿ç»­é‡å¤ç”Ÿæˆ")
        
        print("âœ… é‡å¤æ§åˆ¶æµ‹è¯•é€šè¿‡")

if __name__ == "__main__":
    tester = ModelDecoderTest()
    tester.test_decoding_termination()
    tester.test_repetition_control()